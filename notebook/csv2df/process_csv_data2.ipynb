{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import chardet\n",
    "from darwinutils.log import get_task_logger\n",
    "from darwinutils.mapreduce import parallel_starmap_p\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "logger = get_task_logger(__name__)\n",
    "\n",
    "global_df_lst = []\n",
    "\n",
    "class CSV2DF(object):\n",
    "    def __init__(self, max_byte_num_for_coding_detect=100*1024, max_thread_num=None, csv_max_read_lines = 50000):\n",
    "        if max_thread_num is None:\n",
    "            self._max_thread_num = max(os.cpu_count() - 4, 3)\n",
    "        else:\n",
    "            self._max_thread_num = max_thread_num\n",
    "        self._max_byte_num_for_coding_detect = max_byte_num_for_coding_detect\n",
    "        self._csv_max_read_lines = csv_max_read_lines\n",
    "\n",
    "    def detect_coder(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            if os.stat(file_path).st_size <= self._max_byte_num_for_coding_detect:\n",
    "                detector = chardet.detect(f.read())\n",
    "            else:\n",
    "                detector = chardet.detect(f.read(self._max_byte_num_for_coding_detect))\n",
    "        \"\"\"There are some issues while using gb2312 so change to gb18030\"\"\"\n",
    "        if 'gb2312' == detector['encoding'].lower():\n",
    "            detector['encoding'] = 'gb18030'\n",
    "        logger.debug(\"Detected Coder info is {}\".format(detector))\n",
    "        return detector['encoding']\n",
    "\n",
    "    def read_csv_file(self, file_path, encoding, skiprows=None, read_rows=None, usecols=None):\n",
    "        func = pd.read_csv\n",
    "        try:\n",
    "            file_df = func(file_path, encoding=encoding, skiprows=skiprows, nrows=read_rows,low_memory=False, usecols=usecols)\n",
    "        except Exception as e:\n",
    "            \"\"\"Normally, it caused by out of range for skiprows\"\"\"\n",
    "            print(\"{} - {} may cause out of range for file {}. Reason({})\".format(\n",
    "                skiprows, read_rows, os.path.basename(file_path), str(e)))\n",
    "            file_df = None\n",
    "        return file_df\n",
    "\n",
    "    def map_column_name_idx(self, column_name_lst, map_column_name_lst):\n",
    "        map_column_idx = []\n",
    "        for column_name in map_column_name_lst:\n",
    "            map_column_idx.append(column_name_lst.index(column_name))\n",
    "        map_column_idx.sort()\n",
    "        return map_column_idx\n",
    "            \n",
    "    def read_content(self, file_path, usecols=None, encoding=None):\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(\"{} does not exist\".format(file_path))\n",
    "            return None\n",
    "        if encoding is None:\n",
    "            coder = self.detect_coder(file_path)\n",
    "        else:\n",
    "            coder = encoding\n",
    "        skiprows = 0\n",
    "        \"\"\"Get Header\"\"\"\n",
    "        header = self.read_csv_file(file_path, encoding=coder, skiprows=skiprows, read_rows=2)\n",
    "        columns_name = header.columns.tolist()\n",
    "        all_df = []\n",
    "        print(\"Start Read: Coder:{}\".format(coder))\n",
    "        loop_num = 0\n",
    "        if usecols is not None:\n",
    "            if len(set(usecols).intersection(set(columns_name))) != len(usecols):\n",
    "                print(\"Error: Wrong usecols setting: {} not in list\".format(set(usecols).difference(set(usecols).intersection(set(columns_name)))))\n",
    "                return None         \n",
    "            tmp_cols = usecols\n",
    "            usecols = self.map_column_name_idx(columns_name, usecols)    \n",
    "            columns_name = list(map(lambda s:columns_name[s], usecols))\n",
    "\n",
    "        while True:\n",
    "            param_lst = []\n",
    "            print(\"Start Batch Read\")\n",
    "            for cnt in range(self._max_thread_num):\n",
    "                param_lst.append((file_path, coder, skiprows, self._csv_max_read_lines, usecols))\n",
    "                skiprows += self._csv_max_read_lines\n",
    "            file_df_lst = parallel_starmap_p(self.read_csv_file, param_lst)\n",
    "            file_df_lst = list(file_df_lst)\n",
    "            print(\"Batch Read Done\")\n",
    "            if file_df_lst[-1] is None:\n",
    "                \"\"\"Read complete\"\"\"\n",
    "                all_df.extend(list(filter(lambda s: s is not None, file_df_lst)))\n",
    "                break\n",
    "            else:\n",
    "                all_df.extend(file_df_lst)\n",
    "                if file_df_lst[-1].shape[0] != self._csv_max_read_lines:\n",
    "                    \"\"\"Read complete\"\"\"\n",
    "                    break\n",
    "            loop_num += 1\n",
    "        print(\"Merge {} pieces of DF together. Total Loop: {}\".format(len(all_df), loop_num))\n",
    "        if len(all_df) > 1:\n",
    "            for df in all_df:\n",
    "                df.columns = columns_name\n",
    "        return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darwinutils.mapreduce import parallel_starmap_t\n",
    "import collections\n",
    "import datetime\n",
    "\n",
    "class DF_CLEAN:\n",
    "    def __init__(self, head_flag=True):\n",
    "        if len(global_df_lst) > 0:\n",
    "            if head_flag:            \n",
    "                self._columns = global_df_lst[0].columns.tolist()            \n",
    "            else:\n",
    "                slef._columns = [\"c%04d\" % x for x in range(len(global_df_lst[0].columns))]\n",
    "                for df in global_df_lst:\n",
    "                    df.columns = self._columns\n",
    "            self._columns_dtype_dict = global_df_lst[0].columns.to_series().groupby(global_df_lst[0].dtypes).groups\n",
    "        else:\n",
    "            self._columns = None\n",
    "        self._working_columns = []  # seems pool thread does not support a very long parameter list, have to use this method\n",
    "        self._missing_value_columns = []\n",
    "        self._factor_unknown = \"unknown\"\n",
    "        self._factor_map_dict = collections.defaultdict(dict)\n",
    "         \n",
    "    def _get_missing_value_column_name_lst(self, df_idx):\n",
    "        missing_value_column_name = []\n",
    "        #print(\"Processing {} DF\".format(df_idx))\n",
    "        for name in self._working_columns:\n",
    "            if global_df_lst[df_idx][name].isnull().any():\n",
    "                missing_value_column_name.append(name)\n",
    "        return missing_value_column_name\n",
    "    \n",
    "    def _reduce_lst(self, columns_lst, axis=0, method='union'):\n",
    "        # axis 0 means row dealwith\n",
    "        # axis 1 means column dealwith\n",
    "        # method can use union and intersection, difference and so on\n",
    "        value_c_lst = []\n",
    "        if axis == 1:\n",
    "            columns_zip = list(zip(*columns_lst))\n",
    "            print(\"Total {} blocks\".format(len(columns_zip)))\n",
    "            for column_block in columns_zip:\n",
    "                #print(\"Total {} part per block\".format(len(column_block)))\n",
    "                merged_block = set(column_block[0])\n",
    "                for merged_part in column_block[1:]:\n",
    "                    merged_block =eval(\"set.{}\".format(method))(merged_block,set(merged_part)) \n",
    "                value_c_lst.append(list(merged_block))\n",
    "        else:\n",
    "            last_set = set(columns_lst[0])\n",
    "            for columns_info in columns_lst[1:]:\n",
    "                last_set = eval(\"set.{}\".format(method))(last_set,set(columns_info)) \n",
    "            value_c_lst = list(last_set)\n",
    "        return value_c_lst\n",
    "        \n",
    "    def check_missing_value_columns(self, column_names=None, df_num=None):\n",
    "        if column_names is None:\n",
    "            column_names = self._columns\n",
    "        if column_names is None:\n",
    "            print(\"Error: DF dose not have columns\")\n",
    "            return None\n",
    "        param_lst = []\n",
    "        if df_num is None:\n",
    "            df_num = len(global_df_lst)\n",
    "        self._working_columns = column_names\n",
    "        for idx in range(df_num):\n",
    "            param_lst.append([idx])\n",
    "\n",
    "        column_name_list = parallel_starmap_p(self._get_missing_value_column_name_lst, param_lst)\n",
    "        column_name_list = list(column_name_list)\n",
    "        assert(len(column_name_list)==df_num)     \n",
    "        self._missing_value_columns = self._reduce_lst(column_name_list, axis=0, method='union')\n",
    "        return self._missing_value_columns\n",
    "    \n",
    "    def _get_same_value_column_name_lst(self, df_idx):\n",
    "        same_value_column_name = []\n",
    "        #print(\"Processing {} DF\".format(df_idx))\n",
    "        for name in self._working_columns:\n",
    "            #if df_idx == 1:\n",
    "            #    print(\"Check same value for column {}\".format(name))\n",
    "            if global_df_lst[df_idx][name].isnull().all():\n",
    "                #elif global_df_lst[df_idx][name].isna().all():\n",
    "                same_value_column_name.append(name)\n",
    "            else:\n",
    "                tmp_df = global_df_lst[df_idx][name].fillna(self._factor_unknown)\n",
    "                if len(set(tmp_df.values)) == 1:\n",
    "                    same_value_column_name.append(name)\n",
    "            \n",
    "        #if(df_idx == 1):\n",
    "        #    print(same_value_column_name)\n",
    "        return same_value_column_name\n",
    "        \n",
    "    def check_same_value_columns(self, column_names=None, df_num=None):\n",
    "        if column_names is None:\n",
    "            column_names = self._columns\n",
    "        if column_names is None:\n",
    "            print(\"Error: DF dose not have columns\")\n",
    "            return None\n",
    "        param_lst = []\n",
    "        if df_num is None:\n",
    "            df_num = len(global_df_lst)\n",
    "        self._working_columns = column_names\n",
    "        for idx in range(df_num):\n",
    "            param_lst.append([idx])\n",
    "        column_name_list = parallel_starmap_p(self._get_same_value_column_name_lst, param_lst)\n",
    "        column_name_list = list(column_name_list)\n",
    "        #print(column_name_list)\n",
    "        assert(len(column_name_list)==df_num)        \n",
    "        return self._reduce_lst(column_name_list, axis=0, method='intersection')\n",
    "    \n",
    "    def _get_factor_values(self, df_idx):\n",
    "        factor_column_values = []\n",
    "        #if (df_idx == 0):\n",
    "        #    print(\"Calculate Factor of columns: Processing {} DF\".format(df_idx))\n",
    "        #    print(len(self._working_columns))\n",
    "        for name in self._working_columns:\n",
    "            tmp_df = global_df_lst[df_idx][name].fillna(self._factor_unknown)\n",
    "            tmp_lst = list(tmp_df.values)\n",
    "            tmp_lst.append(self._factor_unknown)\n",
    "            factor_column_values.append(list(set(tmp_lst)))\n",
    "\n",
    "        return factor_column_values\n",
    "    \n",
    "    def _change_column_type(self, df_idx, column_type):\n",
    "        #print(\"Change Column Types: Processing {} DF\".format(df_idx))\n",
    "        for name in self._working_columns:\n",
    "            global_df_lst[df_idx][name] = global_df_lst[df_idx][name].astype(column_type)\n",
    "    \n",
    "    def get_factor_candidate_columns(self, column_names=None,df_num=None):        \n",
    "        if df_num is None:\n",
    "            df_num = len(global_df_lst)\n",
    "        if column_names is None:\n",
    "            column_names = self._columns_dtype_dict[np.dtype(object)].values\n",
    "\n",
    "        self._working_columns = column_names\n",
    "        param_lst = []\n",
    "        for idx in range(df_num):\n",
    "            param_lst.append([idx])\n",
    "        column_name_list = parallel_starmap_p(self._get_factor_values, param_lst)\n",
    "        column_name_list = list(column_name_list)\n",
    "\n",
    "        assert(len(column_name_list)==df_num)  \n",
    "        print(\"Reduce List\")\n",
    "        column_info_lst = self._reduce_lst(column_name_list, axis=1, method='union')\n",
    "        return self._working_columns, column_info_lst\n",
    "    \n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "    \n",
    "    ########################################################\n",
    "    def _set_bool_type_for_column(self, df_idx):\n",
    "        for name in self._working_columns:\n",
    "            #if(df_idx == 0):\n",
    "            #    print(\"Column: {} Set to Bool\".format(name))\n",
    "            tmp_df = global_df_lst[df_idx][name].fillna(0)\n",
    "            values = tmp_df.values.tolist()\n",
    "            for idx, value in enumerate(values):\n",
    "                if value != 0:\n",
    "                    values[idx] = 1\n",
    "            global_df_lst[df_idx][name] = values\n",
    "            \n",
    "            \n",
    "    def set_column_to_bool(self, column_names=None,df_num=None):  \n",
    "        if df_num is None:\n",
    "            df_num = len(global_df_lst)\n",
    "        if column_names is None:\n",
    "            print(\"column_names is None\")\n",
    "            return None\n",
    "     \n",
    "        self._working_columns = column_names\n",
    "        param_lst = []\n",
    "        for idx in range(df_num):\n",
    "            param_lst.append([idx])\n",
    "        parallel_starmap_t(self._set_bool_type_for_column, param_lst)   \n",
    "        \n",
    "    def _set_factor_value_for_column(self, df_idx, value_dict):\n",
    "        for name in self._working_columns:\n",
    "            #if(df_idx == 0):\n",
    "            #    print(\"Column: {} Set to Factor Value\".format(name))\n",
    "            tmp_df = global_df_lst[df_idx][name].fillna(self._factor_unknown)\n",
    "            values = tmp_df.values.tolist()\n",
    "            for idx, value in enumerate(values):\n",
    "                try:\n",
    "                    tmp_value = str(int(float(value)))\n",
    "                    values[idx] = value_dict[name].get(tmp_value)\n",
    "                except ValueError:\n",
    "                    values[idx] = value_dict[name].get(value)\n",
    "                if values[idx] is None:\n",
    "                    print(\"Column {} Value {} not defined\".format(name, str(value)))\n",
    "                    values[idx] = value_dict[name][self._factor_unknown]\n",
    "                    value_dict[name][value] = value_dict[name][self._factor_unknown]\n",
    "            global_df_lst[df_idx][name] = values\n",
    "    \n",
    "    def set_column_to_factor(self, column_names=None,df_num=None, value_dict=None):  \n",
    "        if df_num is None:\n",
    "            df_num = len(global_df_lst)\n",
    "        if column_names is None:\n",
    "            print(\"column_names is None\")\n",
    "            return None\n",
    "        if value_dict is None:\n",
    "            print(\"value_dict is None\")\n",
    "            return None\n",
    "     \n",
    "        self._working_columns = column_names\n",
    "        param_lst = []\n",
    "        for idx in range(df_num):\n",
    "            param_lst.append([idx, value_dict])\n",
    "        parallel_starmap_t(self._set_factor_value_for_column, param_lst)   \n",
    "        \n",
    "    def _drop_columns(self, df_idx):\n",
    "        global_df_lst[df_idx] = global_df_lst[df_idx].drop(self._working_columns, axis=1)\n",
    "    \n",
    "    def drop_columns(self, column_names=None,df_num=None):  \n",
    "        if df_num is None:\n",
    "            df_num = len(global_df_lst)\n",
    "        if column_names is None:\n",
    "            print(\"column_names is None\")\n",
    "            return None\n",
    "     \n",
    "        self._working_columns = column_names\n",
    "        param_lst = []\n",
    "        for idx in range(df_num):\n",
    "            param_lst.append([idx])\n",
    "        parallel_starmap_t(self._drop_columns, param_lst)   \n",
    "        \n",
    "    def _compare_two_columns(self, df_idx, src_column, target_column):\n",
    "        rst = np.where(global_df_lst[df_idx][src_column] == global_df_lst[df_idx][target_column], True, False)\n",
    "        return rst.all()\n",
    "    \n",
    "    def compare_two_columns(self, src_column, target_column, df_num=None):\n",
    "        if df_num is None:\n",
    "            df_num = len(global_df_lst)\n",
    "        param_lst = []\n",
    "        for idx in range(df_num):\n",
    "            param_lst.append([idx, src_column, target_column])\n",
    "        column_rst_list = parallel_starmap_p(self._compare_two_columns, param_lst)   \n",
    "        column_rst_list = list(column_rst_list)\n",
    "        return np.array(column_rst_list).all()\n",
    "    \n",
    "    def _transfer_datetime_to_int(self, df_idx, datetime_format, basetime, scale, prefix):\n",
    "        print_flag = 1\n",
    "        if prefix is None:\n",
    "            prefix = ''\n",
    "        for name in self._working_columns:\n",
    "            if(df_idx == 0):\n",
    "                print(\"Transfer Time on: {}\".format(name))\n",
    "            tmp_df = global_df_lst[df_idx][name].fillna(0)\n",
    "            values = tmp_df.values.tolist()\n",
    "            for idx, value in enumerate(values):\n",
    "                if value == '0':\n",
    "                    values[idx] = 0\n",
    "                    continue\n",
    "                if value != 0:\n",
    "                    # Remove in future\n",
    "                    try:\n",
    "                        tmp_value = str(int(value))\n",
    "                    except ValueError:\n",
    "                        tmp_value = str(value)\n",
    "                    if tmp_value == '' or tmp_value.find(' ')==0:\n",
    "                        values[idx] = 0\n",
    "                        continue\n",
    "                    tmp_value=prefix+tmp_value  \n",
    "\n",
    "                    try:\n",
    "                        tmp_dateitme = datetime.datetime.strptime(tmp_value, datetime_format)\n",
    "                    except Exception as e:\n",
    "                        print(\"Error while transfer string to datatime in column {}, line {} value {}. Reason: {}\".format(name, idx, value, str(e)))\n",
    "                        values[idx] = 0\n",
    "                        continue\n",
    "                    \n",
    "                    if scale==\"Day\":\n",
    "                        values[idx] = (tmp_dateitme - basetime).days\n",
    "                    elif scale==\"Hour\":\n",
    "                        values[idx] = (tmp_dateitme - basetime).days*24 + int((tmp_dateitme - basetime).seconds/3600)\n",
    "                    elif scale==\"Minute\":\n",
    "                        values[idx] = (tmp_dateitme - basetime).days*24*60 + int((tmp_dateitme - basetime).seconds/60)\n",
    "                    elif scale==\"Second\":\n",
    "                        values[idx] = (tmp_dateitme - basetime).total_seconds\n",
    "                    else:\n",
    "                        print(\"Unsupported scale {}\".format(scale))\n",
    "                        return None\n",
    "            if(df_idx == 0):\n",
    "                print(values[:10])\n",
    "            global_df_lst[df_idx][name] = values\n",
    "            \n",
    "        \n",
    "    def transfer_datetime_to_int(self, column_names=None, datetime_format=None, basetime=None, scale=None, df_num=None, prefix=None):\n",
    "        if df_num is None:\n",
    "            df_num = len(global_df_lst)\n",
    "        if column_names is None:\n",
    "            print(\"column_names is None\")\n",
    "            return None\n",
    "        if basetime is None or type(basetime).__name__ != 'datetime':\n",
    "            print(\"basetime is None or format is not datetime\")\n",
    "            return None\n",
    "        if datetime_format is None:\n",
    "            print(\"datetime_format is None\")\n",
    "            return None\n",
    "        if scale is None or scale not in [\"Day\", \"Hour\", \"Minute\", \"Second\"]:\n",
    "            print(\"scale is None or value is not right\")\n",
    "            return None\n",
    "        self._working_columns = column_names\n",
    "        param_lst = []\n",
    "        for idx in range(df_num):\n",
    "            param_lst.append([idx, datetime_format, basetime, scale, prefix])\n",
    "        parallel_starmap_t(self._transfer_datetime_to_int, param_lst)   \n",
    "        \n",
    "    def _update_value_difference_by_columns(self, df_num, refer_column_name, new_column_names=None):\n",
    "        if new_column_names is None:\n",
    "            for name in self._working_columns:\n",
    "                #if(df_idx == 0):\n",
    "                #    print(\"Transfer Time on: {} Set to Factor Value\".format(name))\n",
    "                global_df_lst[df_idx][name] = global_df_lst[df_idx][name] - global_df_lst[df_idx][refer_column_name]\n",
    "        else:\n",
    "            for idx, name in self._working_columns:\n",
    "                #if(df_idx == 0):\n",
    "                #    print(\"Transfer Time on: {} Set to Factor Value\".format(name))\n",
    "                global_df_lst[df_idx][new_column_names[idx]] = global_df_lst[df_idx][name] - global_df_lst[df_idx][refer_column_name]\n",
    "        \n",
    "    def update_value_difference_by_columns(self, refer_column_name, columns_names, new_column_names=None, df_num=None):\n",
    "        if df_num is None:\n",
    "            df_num = len(global_df_lst)\n",
    "        if column_names is None:\n",
    "            print(\"column_names is None\")\n",
    "            return None\n",
    "        if type(refer_column_name).__name__ != 'str':\n",
    "            print(\"refer_column_name type error\")\n",
    "            return None\n",
    "        if new_column_names is not None and len(new_column_names) != len(columns_names):\n",
    "            print(\"new_column_names param is not correct\")\n",
    "            return None\n",
    "        \n",
    "        self._working_columns = column_names\n",
    "        param_lst = []\n",
    "        for idx in range(df_num):\n",
    "            param_lst.append([idx, refer_column_name, new_column_names])\n",
    "        parallel_starmap_t(self._update_value_difference_by_columns, param_lst)   \n",
    "        \n",
    "    def _fill_value_to_nan_cell(self, df_idx, value): \n",
    "        for name in self._working_columns:\n",
    "            if df_idx==0:\n",
    "                print(\"Filling column {}\".format(name))\n",
    "            global_df_lst[df_idx][name] = global_df_lst[df_idx][name].fillna(value)\n",
    "    \n",
    "    def fill_value_to_nan_cell(self, column_names=None, value=0, df_num=None):\n",
    "        if df_num is None:\n",
    "            df_num = len(global_df_lst)\n",
    "        if column_names is None:\n",
    "            column_names = self._columns\n",
    "        self._working_columns = column_names\n",
    "        param_lst = []\n",
    "        for idx in range(df_num):\n",
    "            param_lst.append([idx, value])\n",
    "        parallel_starmap_t(self._fill_value_to_nan_cell, param_lst)   \n",
    "        \n",
    "    def fill_fact_map_dict(self, column_names=None, factor_value=None):\n",
    "        if column_names is None or factor_value is None:\n",
    "            print(\"Wrong input parameter\")\n",
    "            return None\n",
    "        if len(column_names) != len(factor_value):\n",
    "            print(\"Column name list length {} not equate to factor value list length {}\".format(len(column_names),len(factor_value)))\n",
    "            return None\n",
    "        exist_column_dict_name = self._factor_map_dict.keys()\n",
    "        if len(exist_column_dict_name) == 0:\n",
    "            new_column_lst = column_names\n",
    "        else:\n",
    "            new_column_lst = list(set(column_names).difference(set(exist_column_dict_name)))\n",
    "        new_column_lst = list(map(lambda s: column_names.index(s), new_column_lst))\n",
    "        for column_idx in new_column_lst:\n",
    "            information =  list(factor_value[column_idx])\n",
    "            tmp_lst = []\n",
    "            for v in information:\n",
    "                if v == ' ' or v =='':\n",
    "                    continue\n",
    "                try:\n",
    "                    tmp_lst.append(str(int(float(v))))\n",
    "                except:\n",
    "                    tmp_lst.append(str(v))\n",
    "            information = list(set(tmp_lst))\n",
    "            del information[information.index(self._factor_unknown)]\n",
    "            self._factor_map_dict[column_names[column_idx]][self._factor_unknown] = 0\n",
    "            information.sort()\n",
    "            for idx, key in enumerate(information):\n",
    "                self._factor_map_dict[column_names[column_idx]][key] = idx+1\n",
    "        exit_column_lst = set(column_names).intersection(set(exist_column_dict_name))\n",
    "        exit_column_lst = list(map(lambda s: column_names.index(s), exit_column_lst))\n",
    "        for column_idx in exit_column_lst:\n",
    "            information =  list(factor_value[column_idx])\n",
    "            tmp_lst = []\n",
    "            for v in information:\n",
    "                if v == ' ' or v =='':\n",
    "                    continue\n",
    "                try:\n",
    "                    tmp_lst.append(str(int(float(v))))\n",
    "                except:\n",
    "                    tmp_lst.append(str(v))\n",
    "            information = list(set(tmp_lst))\n",
    "            information.sort()\n",
    "            current_num = max(list(self._factor_map_dict[column_names[column_idx]].values())) + 1\n",
    "            for idx, key in enumerate(information):\n",
    "                if self._factor_map_dict[column_names[column_idx]].get(key) is None:\n",
    "                    self._factor_map_dict[column_names[column_idx]][key] = current_num\n",
    "                    current_num += 1\n",
    "        return self._factor_map_dict\n",
    "    \n",
    "    def _get_df_by_value_lst(self, df_idx, column_name, reverse_flag, values):\n",
    "        print(\"Process get_df_by_value. df_idx {}\".format(df_idx))\n",
    "        if reverse_flag:\n",
    "            return global_df_lst[df_idx][~global_df_lst[df_idx][column_name].isin(values)]\n",
    "        else:\n",
    "            return global_df_lst[df_idx][global_df_lst[df_idx][column_name].isin(values)]\n",
    "        \n",
    "        \n",
    "    def get_df_by_value_lst(self, column_name=None, values=None, reverse_flag=False, df_num=None, concat_flag=True):\n",
    "        if column_name is None or type(column_name).__name__ != 'str':\n",
    "            print(\"Wrong column_name parameter\")\n",
    "            return None\n",
    "        if values is None or type(values).__name__ != 'list':\n",
    "            print(\"Wrong values parameter\")\n",
    "            return None\n",
    "        if df_num is None:\n",
    "            df_num = len(global_df_lst)\n",
    "        param_lst = []\n",
    "        for idx in range(df_num):\n",
    "            param_lst.append([idx, column_name, reverse_flag, values])\n",
    "        df_lst = parallel_starmap_t(self._get_df_by_value_lst, param_lst)   \n",
    "        df_lst = list(df_lst)\n",
    "        return_df = None\n",
    "        for df in df_lst:\n",
    "            if not df.empty:\n",
    "                if concat_flag:\n",
    "                    if return_df is None:\n",
    "                        return_df = df\n",
    "                    else:\n",
    "                        return_df = pd.concat([return_df,df],ignore_index=True)\n",
    "                else:\n",
    "                    if return_df is None:\n",
    "                        return_df = [df]\n",
    "                    else:\n",
    "                        return_df.append(df)\n",
    "        return return_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Read: Coder:ascii\n",
      "Start Batch Read\n",
      "Batch Read Done\n",
      "Start Batch Read\n",
      "Batch Read Done\n",
      "Start Batch Read\n",
      "4050000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "4000000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "4150000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3900000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3750000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "4100000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3850000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3650000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3300000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3400000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3250000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3100000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3950000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3450000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3200000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3500000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3350000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3800000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3000000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3700000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3050000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "2950000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3600000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3550000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "3150000 - 50000 may cause out of range for file csv201803_clear_2.csv. Reason(No columns to parse from file)\n",
      "Batch Read Done\n",
      "Merge 59 pieces of DF together. Total Loop: 2\n",
      "Missing:\n",
      "[]\n",
      "Type of Columns: [dtype('int64'), dtype('float64'), dtype('O')]\n",
      "Index(['UUID'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "test_read_csv = CSV2DF()\n",
    "global_df_lst = test_read_csv.read_content(\"csv201803_clear_2.csv\")\n",
    "clean_df = DF_CLEAN()\n",
    "missing_columns = clean_df.check_missing_value_columns()\n",
    "print(\"Missing:\\n{}\".format(missing_columns))\n",
    "print(\"Type of Columns: {}\".format(list(clean_df._columns_dtype_dict.keys())))\n",
    "print(clean_df._columns_dtype_dict[np.dtype(object)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv(\"label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_type_11 = label_df.loc[label_df['month']==201803]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597\n",
      "Process get_df_by_value. df_idx 0Process get_df_by_value. df_idx 1\n",
      "Process get_df_by_value. df_idx 2\n",
      "Process get_df_by_value. df_idx 3\n",
      "Process get_df_by_value. df_idx 4\n",
      "Process get_df_by_value. df_idx 5\n",
      "\n",
      "Process get_df_by_value. df_idx 6Process get_df_by_value. df_idx 7\n",
      "\n",
      "Process get_df_by_value. df_idx 8\n",
      "Process get_df_by_value. df_idx 9\n",
      "Process get_df_by_value. df_idx 10\n",
      "Process get_df_by_value. df_idx 11\n",
      "Process get_df_by_value. df_idx 12\n",
      "Process get_df_by_value. df_idx 13\n",
      "Process get_df_by_value. df_idx 14\n",
      "Process get_df_by_value. df_idx 15\n",
      "Process get_df_by_value. df_idx 16\n",
      "Process get_df_by_value. df_idx 17\n",
      "Process get_df_by_value. df_idx 18\n",
      "Process get_df_by_value. df_idx 19\n",
      "Process get_df_by_value. df_idx 20\n",
      "Process get_df_by_value. df_idx 21\n",
      "Process get_df_by_value. df_idx 22\n",
      "Process get_df_by_value. df_idx 23\n",
      "Process get_df_by_value. df_idx 24\n",
      "Process get_df_by_value. df_idx 25\n",
      "Process get_df_by_value. df_idx 26\n",
      "Process get_df_by_value. df_idx 27\n",
      "Process get_df_by_value. df_idx 28\n",
      "Process get_df_by_value. df_idx 29\n",
      "Process get_df_by_value. df_idx 30\n",
      "Process get_df_by_value. df_idx 31\n",
      "Process get_df_by_value. df_idx 32\n",
      "Process get_df_by_value. df_idx 33\n",
      "Process get_df_by_value. df_idx 34\n",
      "Process get_df_by_value. df_idx 35\n",
      "Process get_df_by_value. df_idx 36\n",
      "Process get_df_by_value. df_idx 37\n",
      "Process get_df_by_value. df_idx 38\n",
      "Process get_df_by_value. df_idx 39\n",
      "Process get_df_by_value. df_idx 40\n",
      "Process get_df_by_value. df_idx 41\n",
      "Process get_df_by_value. df_idx 42\n",
      "Process get_df_by_value. df_idx 43\n",
      "Process get_df_by_value. df_idx 44\n",
      "Process get_df_by_value. df_idx 45\n",
      "Process get_df_by_value. df_idx 46\n",
      "Process get_df_by_value. df_idx 47\n",
      "Process get_df_by_value. df_idx 48\n",
      "Process get_df_by_value. df_idx 49\n",
      "Process get_df_by_value. df_idx 50\n",
      "Process get_df_by_value. df_idx 51\n",
      "Process get_df_by_value. df_idx 52Process get_df_by_value. df_idx 53\n",
      "\n",
      "Process get_df_by_value. df_idx 54\n",
      "Process get_df_by_value. df_idx 55\n",
      "Process get_df_by_value. df_idx 56\n",
      "Process get_df_by_value. df_idx 57\n",
      "Process get_df_by_value. df_idx 58\n"
     ]
    }
   ],
   "source": [
    "clean_df = DF_CLEAN()\n",
    "month_type_11_uuid = list(month_type_11['UUID'].values)\n",
    "print(len(month_type_11_uuid))\n",
    "rst = clean_df.get_df_by_value_lst(column_name='UUID', values=month_type_11_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = [1]*len(rst.index)\n",
    "rst['label'] = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "rst_idx = []\n",
    "while len(rst_idx) < 100:\n",
    "    size = 100 - len(rst_idx)\n",
    "    rst_idx.extend(list(np.random.randint(len(rst.index), size=(size))))\n",
    "    rst_idx = list(set(rst_idx))\n",
    "test_df = pd.DataFrame(rst, index=rst_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"csv201803_test_type1.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497\n"
     ]
    }
   ],
   "source": [
    "train_idx = list(set(range(len(rst.index))).difference(set(rst_idx)))\n",
    "print(len(train_idx))\n",
    "train_df = pd.DataFrame(rst, index=train_idx)\n",
    "train_df.to_csv(\"csv201803_train_type1.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process get_df_by_value. df_idx 0\n",
      "Process get_df_by_value. df_idx 1\n",
      "Process get_df_by_value. df_idx 2\n",
      "Process get_df_by_value. df_idx 3\n",
      "Process get_df_by_value. df_idx 4\n",
      "Process get_df_by_value. df_idx 5\n",
      "Process get_df_by_value. df_idx 6\n",
      "Process get_df_by_value. df_idx 7\n",
      "Process get_df_by_value. df_idx 8\n",
      "Process get_df_by_value. df_idx 9\n",
      "Process get_df_by_value. df_idx 10\n",
      "Process get_df_by_value. df_idx 11\n",
      "Process get_df_by_value. df_idx 12\n",
      "Process get_df_by_value. df_idx 13\n",
      "Process get_df_by_value. df_idx 14\n",
      "Process get_df_by_value. df_idx 15\n",
      "Process get_df_by_value. df_idx 16\n",
      "Process get_df_by_value. df_idx 17\n",
      "Process get_df_by_value. df_idx 18\n",
      "Process get_df_by_value. df_idx 19\n",
      "Process get_df_by_value. df_idx 20\n",
      "Process get_df_by_value. df_idx 21\n",
      "Process get_df_by_value. df_idx 22\n",
      "Process get_df_by_value. df_idx 23\n",
      "Process get_df_by_value. df_idx 24Process get_df_by_value. df_idx 25\n",
      "\n",
      "Process get_df_by_value. df_idx 26\n",
      "Process get_df_by_value. df_idx 27\n",
      "Process get_df_by_value. df_idx 28\n",
      "Process get_df_by_value. df_idx 29\n",
      "Process get_df_by_value. df_idx 30\n",
      "Process get_df_by_value. df_idx 31\n",
      "Process get_df_by_value. df_idx 32\n",
      "Process get_df_by_value. df_idx 33\n",
      "Process get_df_by_value. df_idx 34\n",
      "Process get_df_by_value. df_idx 35\n",
      "Process get_df_by_value. df_idx 36\n",
      "Process get_df_by_value. df_idx 37\n",
      "Process get_df_by_value. df_idx 38Process get_df_by_value. df_idx 39\n",
      "Process get_df_by_value. df_idx 40\n",
      "\n",
      "Process get_df_by_value. df_idx 41\n",
      "Process get_df_by_value. df_idx 42\n",
      "Process get_df_by_value. df_idx 43\n",
      "Process get_df_by_value. df_idx 44\n",
      "Process get_df_by_value. df_idx 45\n",
      "Process get_df_by_value. df_idx 46\n",
      "Process get_df_by_value. df_idx 47Process get_df_by_value. df_idx 48\n",
      "\n",
      "Process get_df_by_value. df_idx 49\n",
      "Process get_df_by_value. df_idx 50\n",
      "Process get_df_by_value. df_idx 51\n",
      "Process get_df_by_value. df_idx 52\n",
      "Process get_df_by_value. df_idx 53\n",
      "Process get_df_by_value. df_idx 54\n",
      "Process get_df_by_value. df_idx 55\n",
      "Process get_df_by_value. df_idx 56\n",
      "Process get_df_by_value. df_idx 57Process get_df_by_value. df_idx 58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rst = clean_df.get_df_by_value_lst(column_name='UUID', values=month_type_11_uuid, reverse_flag=True, concat_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2928242\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for df in rst:\n",
    "    #print(len(df.index))\n",
    "    total += len(df.index)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write df 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write df 1\n",
      "Write df 2\n",
      "Write df 3\n",
      "Write df 4\n",
      "Write df 5\n",
      "Write df 6\n",
      "Write df 7\n",
      "Write df 8\n",
      "Write df 9\n",
      "Write df 10\n",
      "Write df 11\n",
      "Write df 12\n",
      "Write df 13\n",
      "Write df 14\n",
      "Write df 15\n",
      "Write df 16\n",
      "Write df 17\n",
      "Write df 18\n",
      "Write df 19\n",
      "Write df 20\n",
      "Write df 21\n",
      "Write df 22\n",
      "Write df 23\n",
      "Write df 24\n",
      "Write df 25\n",
      "Write df 26\n",
      "Write df 27\n",
      "Write df 28\n",
      "Write df 29\n",
      "Write df 30\n",
      "Write df 31\n",
      "Write df 32\n",
      "Write df 33\n",
      "Write df 34\n",
      "Write df 35\n",
      "Write df 36\n",
      "Write df 37\n",
      "Write df 38\n",
      "Write df 39\n",
      "Write df 40\n",
      "Write df 41\n",
      "Write df 42\n",
      "Write df 43\n",
      "Write df 44\n",
      "Write df 45\n",
      "Write df 46\n",
      "Write df 47\n",
      "Write df 48\n",
      "Write df 49\n",
      "Write df 50\n",
      "Write df 51\n",
      "Write df 52\n",
      "Write df 53\n",
      "Write df 54\n",
      "Write df 55\n",
      "Write df 56\n",
      "Write df 57\n",
      "Write df 58\n"
     ]
    }
   ],
   "source": [
    "with open(\"csv201803_train.csv\", \"w\") as f:\n",
    "    train_df.to_csv(f, mode=\"w\", index=False, header=True)     \n",
    "    each_df_times = 4\n",
    "    each_sample_num = len(train_df.index)*5\n",
    "    for cnt, df in enumerate(rst):\n",
    "        print(\"Write df {}\".format(cnt))\n",
    "        label_id = [0]*len(df.index)\n",
    "        df['label'] = label_id\n",
    "        rst_idx = []\n",
    "        while len(rst_idx) < each_sample_num*each_df_times:\n",
    "            size = each_sample_num*each_df_times - len(rst_idx)\n",
    "            rst_idx.extend(list(np.random.randint(len(df.index), size=(size))))\n",
    "            rst_idx = list(set(rst_idx))\n",
    "        window_size = int(round(len(rst_idx)/each_df_times))\n",
    "        start_point = 0\n",
    "        for idx in range(each_df_times):\n",
    "            tmp_df = pd.DataFrame(df, index=rst_idx[start_point:min(start_point+window_size, len(rst_idx))])\n",
    "            tmp_df.to_csv(f, mode=\"a\", index=False, header=False)\n",
    "            train_df.to_csv(f, mode=\"a\", index=False, header=False)     \n",
    "            start_point += window_size\n",
    "            if start_point >= len(rst_idx):\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write df 0\n",
      "Write df 1\n",
      "Write df 2\n",
      "Write df 3\n",
      "Write df 4\n",
      "Write df 5\n",
      "Write df 6\n",
      "Write df 7\n",
      "Write df 8\n",
      "Write df 9\n",
      "Write df 10\n",
      "Write df 11\n",
      "Write df 12\n",
      "Write df 13\n",
      "Write df 14\n",
      "Write df 15\n",
      "Write df 16\n",
      "Write df 17\n",
      "Write df 18\n",
      "Write df 19\n",
      "Write df 20\n",
      "Write df 21\n",
      "Write df 22\n",
      "Write df 23\n",
      "Write df 24\n",
      "Write df 25\n",
      "Write df 26\n",
      "Write df 27\n",
      "Write df 28\n",
      "Write df 29\n",
      "Write df 30\n",
      "Write df 31\n",
      "Write df 32\n",
      "Write df 33\n",
      "Write df 34\n",
      "Write df 35\n",
      "Write df 36\n",
      "Write df 37\n",
      "Write df 38\n",
      "Write df 39\n",
      "Write df 40\n",
      "Write df 41\n",
      "Write df 42\n",
      "Write df 43\n",
      "Write df 44\n",
      "Write df 45\n",
      "Write df 46\n",
      "Write df 47\n",
      "Write df 48\n",
      "Write df 49\n",
      "Write df 50\n",
      "Write df 51\n",
      "Write df 52\n",
      "Write df 53\n",
      "Write df 54\n",
      "Write df 55\n",
      "Write df 56\n",
      "Write df 57\n",
      "Write df 58\n"
     ]
    }
   ],
   "source": [
    "with open(\"csv201803_test.csv\", \"w\") as f:\n",
    "    test_df.to_csv(f, mode=\"w\", index=False, header=True)     \n",
    "    each_df_times = 1\n",
    "    each_sample_num = 100\n",
    "    for cnt, df in enumerate(rst):\n",
    "        print(\"Write df {}\".format(cnt))\n",
    "        rst_idx = []\n",
    "        while len(rst_idx) < 100:\n",
    "            size = 100 - len(rst_idx)\n",
    "            rst_idx.extend(list(np.random.randint(len(df.index), size=(size))))\n",
    "            rst_idx = list(set(rst_idx))\n",
    "\n",
    "        tmp_df = pd.DataFrame(df, index=rst_idx)\n",
    "        tmp_df.to_csv(f, mode=\"a\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 UUID  OUUID  ORD_NO   CLR_DT  RE_BUS_CNL  \\\n",
      "1    97d1edeef744463da8ed8c3dc382b32d      0       0  30240.0           2   \n",
      "5    dd39a6fd79e44fe2a355ba1371ea4a8b      0       0  31680.0           2   \n",
      "520  0f6843a9e4eb4f378b9822c0df2a0744      0       0  33120.0           2   \n",
      "17   d9cd34c1003a4d00b4f705e9442d85a6      0       0  33120.0           2   \n",
      "25   d382f0af9c304736b52e7279d4b5db1d      0       0  33120.0           2   \n",
      "31   b88ca558e1834d57a4bead7351817732      0       0  31680.0           2   \n",
      "35   4d328f4068fe4e649f51eaf0cc2e0259      0       0  31680.0           2   \n",
      "40   9c9e4daf7a014863a6ee77d47713fca6      0       0  33120.0           2   \n",
      "47   74bd8aec0560432a9af21fbbbe132f33      0       0  33120.0           2   \n",
      "55   5d38fbcc22474c6faafcf7a40204f9e6      0       0  31680.0           2   \n",
      "56   1064557857e84dbca2cb008fca2e5f52      0       0  31680.0           2   \n",
      "59   8bc353340ea646a8844f159ccb055275      0       0  31680.0           2   \n",
      "67   ff752d517b9b43be989fdb6a743b5fff      0       0  33120.0           2   \n",
      "68   8f0936589337422d9435a887bc1ff277      0       0  33120.0           2   \n",
      "70   56bab65e8186443c90d7ecca35852257      0       0  33120.0           2   \n",
      "76   35e352640f0e40e880f0a2a409807749      0       0  33120.0           2   \n",
      "80   7db313567db94dca8df8cb3f04359168      0       0  30240.0           2   \n",
      "81   62e6b56465b1466bac211b24a410eeb2      0       0  30240.0           2   \n",
      "85   b508c0f05c7f4eb19424de6b642b8c62      0       0  31680.0           2   \n",
      "87   a74386193aa04d4ea27cdeb2b59c7ce8      0       0  31680.0           2   \n",
      "92   8d2df36f111244e4913d67755d0cf307      0       0  31680.0           2   \n",
      "93   f9fcf35f716849eda43c26c01aa9c0ef      0       0  31680.0           2   \n",
      "96   224a9b6a72ad4e65a9dccaac3b5e36d6      0       0  31680.0           2   \n",
      "100  9c61182f9fdd4e99ab856d481f135e1b      0       0  31680.0           2   \n",
      "106  a40be4d186a246458fc03fcf54975678      0       0  33120.0           2   \n",
      "114  749793886643446392df67d8e28a2db9      0       0  33120.0           2   \n",
      "123  3df9158db29345559171669002c19bb1      0       0  33120.0           2   \n",
      "125  bfc60a318868419e9df9065048464c8b      0       0  30240.0           2   \n",
      "138  da12cf1c4eda4d1786c323cd7d486b94      0       0  33120.0           2   \n",
      "142  719ed93df99a4caabe1047091225c7ad      0       0  33120.0           2   \n",
      "..                                ...    ...     ...      ...         ...   \n",
      "358  8b7f7e5266e74ab0bcdc4797b1bbd866      0       0  31680.0           2   \n",
      "360  9b508602057b4b52845be2b948336a78      0       0  31680.0           2   \n",
      "367  8613b3d2201a4b9a8dae3e91416ed185      0       0  33120.0           2   \n",
      "369  829181f4a503404e9596b4021d9a6887      0       0  33120.0           2   \n",
      "373  289480066eac4bffa1f7e666e72ca193      0       0  33120.0           2   \n",
      "390  3c63241712764003b3694a180b7bff43      0       0  31680.0           2   \n",
      "392  5df8a0ca35a943d9868c094c3838cd17      0       0  31680.0           2   \n",
      "396  c4733af945e64dfe99b6743321e87ba7      0       0  31680.0           2   \n",
      "398  f8a8f5f6ffd14db398670fc4b596daef      0       0  31680.0           2   \n",
      "400  5a470e6685f24d48b90250d17dce73ec      0       0  33120.0           2   \n",
      "401  134d98bc39204d2b9ed5844106875a0c      0       0  33120.0           2   \n",
      "403  8a4f5fa311594880b55a99563eff1908      0       0  33120.0           2   \n",
      "408  143d9f983426400580d085c44e83306e      0       0  33120.0           2   \n",
      "414  07853b1327384370bfddcac47f8e9e93      0       0  33120.0           2   \n",
      "418  12c350eac5d54d859bb48f4dc8c0df91      0       0  28800.0           2   \n",
      "423  0bf4fb16d5b54928b931aeb0f551ff97      0       0  31680.0           2   \n",
      "426  06ede8b7726045c3bd3e38b159326d3a      0       0  31680.0           2   \n",
      "443  b6d33b20ded142718f23918d341387bd      0       0  33120.0           2   \n",
      "444  7884695ee65f453f8d8477310425d9a2      0       0  33120.0           2   \n",
      "449  35aa6736c92341fa8afa8eaf2a382f9c      0       0  28800.0           2   \n",
      "459  65c945fa5eb64d9e942a09f1c40c0cdb      0       0  31680.0           2   \n",
      "460  96f786c397874ad4a295315260e7891a      0       0  31680.0           2   \n",
      "466  54276a7ef98a4637a0d3a1e384f956a1      0       0  31680.0           2   \n",
      "470  5e45c1cc36d64d468dea2a9df6ea7571      0       0  33120.0           2   \n",
      "482  3f5bcb3e69e8473e824fa6feeb96f572      0       0  33120.0           2   \n",
      "483  e071680ef28e457eb5bf9932838f28b3      0       0  33120.0           2   \n",
      "488  b3292d14762246d0869db845b9fac1b1      0       0  30240.0           2   \n",
      "491  863b67b674c54747be6bb7bd6e9eb366      0       0  31680.0           2   \n",
      "492  7b9d20cfb82041a992f515e6ad980952      0       0  31680.0           2   \n",
      "506  6f3dc52243c74ac79bb01127c5f03827      0       0  31680.0           2   \n",
      "\n",
      "              IN_MNO  TRM_NO  MCC_CD  BAT_NO  TRAN_DT  ...    \\\n",
      "1    800652651390004     1.0      31       1    30240  ...     \n",
      "5    800314253110016     1.0      40       1    31680  ...     \n",
      "520  800393151370010     1.0      30       1    33120  ...     \n",
      "17   800652656990010     1.0      68       1    33120  ...     \n",
      "25   800652652510049     1.0      37       1    33120  ...     \n",
      "31   800652656210028     1.0      58       1    31680  ...     \n",
      "35   800521051373283     1.0      30       1    31680  ...     \n",
      "40   800103451370012     1.0      30       1    33120  ...     \n",
      "47   800652656210028     1.0      58       1    33120  ...     \n",
      "55   800391258120000     1.0      80       1    31680  ...     \n",
      "56   800652651370147     1.0      30       1    31680  ...     \n",
      "59   800652651370147     1.0      30       1    31680  ...     \n",
      "67   800103451370012     1.0      30       1    33120  ...     \n",
      "68   800652650940168     1.0      26       1    33120  ...     \n",
      "70   800652451370079     1.0      30       1    33120  ...     \n",
      "76   800652650210024     1.0      16       1    33120  ...     \n",
      "80   800652651390004     1.0      31       1    30240  ...     \n",
      "81   800652651390004     1.0      31       1    30240  ...     \n",
      "85   800652651370146     1.0      30       1    31680  ...     \n",
      "87   800652451370078     1.0      30       1    31680  ...     \n",
      "92   800691650720011     1.0      24       1    31680  ...     \n",
      "93   800691650720011     1.0      24       1    31680  ...     \n",
      "96   800652650720016     1.0      24       1    31680  ...     \n",
      "100  800652451370081     1.0      30       1    31680  ...     \n",
      "106  800624659980004     1.0     115       1    33120  ...     \n",
      "114  800691650720011     1.0      24       1    33120  ...     \n",
      "123  800652656210030     1.0      58       1    33120  ...     \n",
      "125  800652651390004     1.0      31       1    30240  ...     \n",
      "138  800652651370150     1.0      30       1    33120  ...     \n",
      "142  800103451370012     1.0      30       1    33120  ...     \n",
      "..               ...     ...     ...     ...      ...  ...     \n",
      "358  800652651370150     1.0      30       1    31680  ...     \n",
      "360  800521051373283     1.0      30       1    31680  ...     \n",
      "367  800103451370012     1.0      30       1    33120  ...     \n",
      "369  800652651370150     1.0      30       1    33120  ...     \n",
      "373  800652459400000     1.0      89       1    33120  ...     \n",
      "390  800314253110016     1.0      40       1    31680  ...     \n",
      "392  800533751370005     1.0      30       1    31680  ...     \n",
      "396  800651551370122     1.0      30       1    31680  ...     \n",
      "398  800652451370079     1.0      30       1    31680  ...     \n",
      "400  800652451370081     1.0      30       1    33120  ...     \n",
      "401  800652656210028     1.0      58       1    33120  ...     \n",
      "403  800103451370012     1.0      30       1    33120  ...     \n",
      "408  800451473920003     1.0     147       1    33120  ...     \n",
      "414  800652656210028     1.0      58       1    33120  ...     \n",
      "418  800491354990111     1.0      49       1    28800  ...     \n",
      "423  800453459980043     1.0     115       1    31680  ...     \n",
      "426  800533751370005     1.0      30       1    31680  ...     \n",
      "443  800652650210024     1.0      16       1    33120  ...     \n",
      "444  800521051373285     1.0      30       1    33120  ...     \n",
      "449  800491354990111     1.0      49       1    28800  ...     \n",
      "459  800652451370078     1.0      30       1    31680  ...     \n",
      "460  800314253110016     1.0      40       1    31680  ...     \n",
      "466  800652451370081     1.0      30       1    31680  ...     \n",
      "470  800491359980054     1.0     115       1    33120  ...     \n",
      "482  800652652510049     1.0      37       1    33120  ...     \n",
      "483  800521051373285     1.0      30       1    33120  ...     \n",
      "488  800652650450008     1.0      19       1    30240  ...     \n",
      "491  800453459980043     1.0     115       1    31680  ...     \n",
      "492  800314253110016     1.0      40       1    31680  ...     \n",
      "506  800652650720016     1.0      24       1    31680  ...     \n",
      "\n",
      "     BANK_TEAM_WORK_SIGN  CUSTOM_CLASSIFY  MANAGER_TEAM_WORK  BANK_MANAGER  \\\n",
      "1                      0              0.0                0.0           0.0   \n",
      "5                      0              0.0                0.0           0.0   \n",
      "520                    0              0.0                0.0           0.0   \n",
      "17                     0              0.0                0.0           0.0   \n",
      "25                     0              0.0                0.0           0.0   \n",
      "31                     0              0.0                0.0           0.0   \n",
      "35                     0              0.0                0.0           0.0   \n",
      "40                     0              0.0                0.0           0.0   \n",
      "47                     0              0.0                0.0           0.0   \n",
      "55                     0              0.0                0.0           0.0   \n",
      "56                     0              0.0                0.0           0.0   \n",
      "59                     0              0.0                0.0           0.0   \n",
      "67                     0              0.0                0.0           0.0   \n",
      "68                     0              0.0                0.0           0.0   \n",
      "70                     0              0.0                0.0           0.0   \n",
      "76                     0              0.0                0.0           0.0   \n",
      "80                     0              0.0                0.0           0.0   \n",
      "81                     0              0.0                0.0           0.0   \n",
      "85                     0              0.0                0.0           0.0   \n",
      "87                     0              0.0                0.0           0.0   \n",
      "92                     0              0.0                0.0           0.0   \n",
      "93                     0              0.0                0.0           0.0   \n",
      "96                     0              0.0                0.0           0.0   \n",
      "100                    0              0.0                0.0           0.0   \n",
      "106                    0              0.0                0.0           0.0   \n",
      "114                    0              0.0                0.0           0.0   \n",
      "123                    0              0.0                0.0           0.0   \n",
      "125                    0              0.0                0.0           0.0   \n",
      "138                    0              0.0                0.0           0.0   \n",
      "142                    0              0.0                0.0           0.0   \n",
      "..                   ...              ...                ...           ...   \n",
      "358                    0              0.0                0.0           0.0   \n",
      "360                    0              0.0                0.0           0.0   \n",
      "367                    0              0.0                0.0           0.0   \n",
      "369                    0              0.0                0.0           0.0   \n",
      "373                    0              0.0                0.0           0.0   \n",
      "390                    0              0.0                0.0           0.0   \n",
      "392                    0              0.0                0.0           0.0   \n",
      "396                    0              0.0                0.0           0.0   \n",
      "398                    0              0.0                0.0           0.0   \n",
      "400                    0              0.0                0.0           0.0   \n",
      "401                    0              0.0                0.0           0.0   \n",
      "403                    0              0.0                0.0           0.0   \n",
      "408                    0              0.0                0.0           0.0   \n",
      "414                    0              0.0                0.0           0.0   \n",
      "418                    0              0.0                0.0           0.0   \n",
      "423                    0              0.0                0.0           0.0   \n",
      "426                    0              0.0                0.0           0.0   \n",
      "443                    0              0.0                0.0           0.0   \n",
      "444                    0              0.0                0.0           0.0   \n",
      "449                    0              0.0                0.0           0.0   \n",
      "459                    0              0.0                0.0           0.0   \n",
      "460                    0              0.0                0.0           0.0   \n",
      "466                    0              0.0                0.0           0.0   \n",
      "470                    0              0.0                0.0           0.0   \n",
      "482                    0              0.0                0.0           0.0   \n",
      "483                    0              0.0                0.0           0.0   \n",
      "488                    0              0.0                0.0           0.0   \n",
      "491                    0              0.0                0.0           0.0   \n",
      "492                    0              0.0                0.0           0.0   \n",
      "506                    0              0.0                0.0           0.0   \n",
      "\n",
      "     MEC_ADMIN_TEL_ENC  CONT_TEL_NO_ENC  LEG_PER_CRD_NO_ENC  LEG_PER_NM_ENC  \\\n",
      "1                    1                1                   1               1   \n",
      "5                    1                1                   1               1   \n",
      "520                  1                1                   1               1   \n",
      "17                   1                1                   1               1   \n",
      "25                   1                1                   1               1   \n",
      "31                   1                1                   1               1   \n",
      "35                   1                1                   1               1   \n",
      "40                   1                1                   1               1   \n",
      "47                   1                1                   1               1   \n",
      "55                   1                1                   1               1   \n",
      "56                   1                1                   1               1   \n",
      "59                   1                1                   1               1   \n",
      "67                   1                1                   1               1   \n",
      "68                   1                1                   1               1   \n",
      "70                   1                1                   1               1   \n",
      "76                   1                1                   1               1   \n",
      "80                   1                1                   1               1   \n",
      "81                   1                1                   1               1   \n",
      "85                   1                1                   1               1   \n",
      "87                   1                1                   1               1   \n",
      "92                   1                1                   1               1   \n",
      "93                   1                1                   1               1   \n",
      "96                   1                1                   1               1   \n",
      "100                  1                1                   1               1   \n",
      "106                  1                1                   1               1   \n",
      "114                  1                1                   1               1   \n",
      "123                  1                1                   1               1   \n",
      "125                  1                1                   1               1   \n",
      "138                  1                1                   1               1   \n",
      "142                  1                1                   1               1   \n",
      "..                 ...              ...                 ...             ...   \n",
      "358                  1                1                   1               1   \n",
      "360                  1                1                   1               1   \n",
      "367                  1                1                   1               1   \n",
      "369                  1                1                   1               1   \n",
      "373                  1                1                   1               1   \n",
      "390                  1                1                   1               1   \n",
      "392                  1                1                   1               1   \n",
      "396                  1                1                   1               1   \n",
      "398                  1                1                   1               1   \n",
      "400                  1                1                   1               1   \n",
      "401                  1                1                   1               1   \n",
      "403                  1                1                   1               1   \n",
      "408                  1                1                   1               1   \n",
      "414                  1                1                   1               1   \n",
      "418                  1                1                   1               1   \n",
      "423                  1                1                   1               1   \n",
      "426                  1                1                   1               1   \n",
      "443                  1                1                   1               1   \n",
      "444                  1                1                   1               1   \n",
      "449                  1                1                   1               1   \n",
      "459                  1                1                   1               1   \n",
      "460                  1                1                   1               1   \n",
      "466                  1                1                   1               1   \n",
      "470                  1                1                   1               1   \n",
      "482                  1                1                   1               1   \n",
      "483                  1                1                   1               1   \n",
      "488                  1                1                   1               1   \n",
      "491                  1                1                   1               1   \n",
      "492                  1                1                   1               1   \n",
      "506                  1                1                   1               1   \n",
      "\n",
      "     OTHER_ENC_FLG  label  \n",
      "1              0.0      2  \n",
      "5              0.0      2  \n",
      "520            0.0      2  \n",
      "17             0.0      2  \n",
      "25             0.0      2  \n",
      "31             0.0      2  \n",
      "35             0.0      2  \n",
      "40             0.0      2  \n",
      "47             0.0      2  \n",
      "55             0.0      2  \n",
      "56             0.0      2  \n",
      "59             0.0      2  \n",
      "67             0.0      2  \n",
      "68             0.0      2  \n",
      "70             0.0      2  \n",
      "76             0.0      2  \n",
      "80             0.0      2  \n",
      "81             0.0      2  \n",
      "85             0.0      2  \n",
      "87             0.0      2  \n",
      "92             0.0      2  \n",
      "93             0.0      2  \n",
      "96             0.0      2  \n",
      "100            0.0      2  \n",
      "106            0.0      2  \n",
      "114            0.0      2  \n",
      "123            0.0      2  \n",
      "125            0.0      2  \n",
      "138            0.0      2  \n",
      "142            0.0      2  \n",
      "..             ...    ...  \n",
      "358            0.0      2  \n",
      "360            0.0      2  \n",
      "367            0.0      2  \n",
      "369            0.0      2  \n",
      "373            0.0      2  \n",
      "390            0.0      2  \n",
      "392            0.0      2  \n",
      "396            0.0      2  \n",
      "398            0.0      2  \n",
      "400            0.0      2  \n",
      "401            0.0      2  \n",
      "403            0.0      2  \n",
      "408            0.0      2  \n",
      "414            0.0      2  \n",
      "418            0.0      2  \n",
      "423            0.0      2  \n",
      "426            0.0      2  \n",
      "443            0.0      2  \n",
      "444            0.0      2  \n",
      "449            0.0      2  \n",
      "459            0.0      2  \n",
      "460            0.0      2  \n",
      "466            0.0      2  \n",
      "470            0.0      2  \n",
      "482            0.0      2  \n",
      "483            0.0      2  \n",
      "488            0.0      2  \n",
      "491            0.0      2  \n",
      "492            0.0      2  \n",
      "506            0.0      2  \n",
      "\n",
      "[100 rows x 135 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
