{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Apr 18 19:32:42 2017\n",
    "\n",
    "@author: raysun\n",
    "\n",
    "Source - https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\"\"\"\n",
    "\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)\n",
    "\n",
    "# Step 1: Import wikipedia\n",
    "import wikipedia\n",
    "\n",
    "#print(wikipedia.summary(\"Wikipedia\"))\n",
    "#print(wikipedia.summary(\"Facebook\", sentences=1))\n",
    "\n",
    "#wikilists = wikipedia.search(\"Trump\")\n",
    "#print(wikilists)\n",
    "\n",
    "#wikipage = wikipedia.page(\"Queen Elizabeth II\")\n",
    "#wikipage = wikipedia.page(\"George Washington\")\n",
    "#wikipage = wikipedia.page(\"Albert Einstein\")\n",
    "#wikipage = wikipedia.page(\"California\")\n",
    "#print(wikipage.title)\n",
    "#print(wikipage.url)\n",
    "#print(wikipage.content)\n",
    "#print(wikipage.links[0])\n",
    "#wikipedia.set_lang(\"fr\")\n",
    "\n",
    "#'title' denotes the exact title of the article to be fetched\n",
    "#title = \"Machine learning\"\n",
    "#wikipage = wikipedia.page(title)\n",
    "#print(wikipage.url)\n",
    "#titles = wikipedia.search('machine learning')\n",
    "#wikipage = wikipedia.page(titles[0])\n",
    "#print(wikipage.title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load wikipedia page and perform tokenization\n",
    "\n",
    "# Read in a wikipedia page for this study\n",
    "\n",
    "wikipage = wikipedia.page(\"Lady Gaga\")\n",
    "\n",
    "documents = wikipage.content\n",
    "\n",
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer()\n",
    "sentences = tokenizer.tokenize(documents.split())\n",
    "#print(sentences)\n",
    "\n",
    "from nltk import corpus\n",
    "stoplist = corpus.stopwords.words(fileids='english')\n",
    "texts = [[word for word in sentence.lower().split() if word not in stoplist]\n",
    "             for sentence in sentences]\n",
    "print(texts[:20])\n",
    "\n",
    "sentences = []\n",
    "for terms in texts:\n",
    "    for term in terms:\n",
    "        if term not in [\",\",\";\",\".\",\"?\",\"\\\"\",\"(\",\")\",\"====\",\"===\",\"==\",\"..\",\":\",u\"\\u2013\"]:\n",
    "            #term = term.lower() # convert to lower case\n",
    "            if term[0] in ['(']:\n",
    "                term = term[1:]\n",
    "            elif term[-1] in [')','.',',',';']:\n",
    "                term = term[:-1]\n",
    "            term.strip().replace(\"=\",\"\")\n",
    "            term.strip().replace(\"-\",\"\")\n",
    "            term.strip().replace(\"'s\",\"\")\n",
    "            if len(term) > 0:\n",
    "                sentences.append(term)\n",
    "print(sentences[:20])\n",
    "\n",
    "# Save it for our word2vec experiments\n",
    "sentences_orig = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Perform stemming\n",
    "# Note that Chinese language may skip this step\n",
    "\n",
    "from gensim.parsing import PorterStemmer\n",
    "global_stemmer = PorterStemmer()\n",
    "     \n",
    "class StemmingHelper(object):\n",
    "    \"\"\"\n",
    "    Class to aid the stemming process - from word to stemmed form,\n",
    "    and vice versa.\n",
    "    The 'original' form of a stemmed word will be returned as the\n",
    "    form in which its been used the most number of times in the text.\n",
    "    \"\"\"\n",
    " \n",
    "    #This reverse lookup will remember the original forms of the stemmed\n",
    "    #words\n",
    "    word_lookup = {}\n",
    " \n",
    "    @classmethod\n",
    "    def stem(cls, word):\n",
    "        \"\"\"\n",
    "        Stems a word and updates the reverse lookup.\n",
    "        \"\"\"\n",
    " \n",
    "        #Stem the word\n",
    "        stemmed = global_stemmer.stem(word)\n",
    " \n",
    "        #Update the word lookup\n",
    "        if stemmed not in cls.word_lookup:\n",
    "            cls.word_lookup[stemmed] = {}\n",
    "        cls.word_lookup[stemmed][word] = (\n",
    "            cls.word_lookup[stemmed].get(word, 0) + 1)\n",
    " \n",
    "        return stemmed\n",
    " \n",
    "    @classmethod\n",
    "    def original_form(cls, word):\n",
    "        \"\"\"\n",
    "        Returns original form of a word given the stemmed version,\n",
    "        as stored in the word lookup.\n",
    "        \"\"\"\n",
    " \n",
    "        if word in cls.word_lookup:\n",
    "            return max(cls.word_lookup[word].keys(),\n",
    "                       key=lambda x: cls.word_lookup[word][x])\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "StemmingHelper.stem('learning')\n",
    "StemmingHelper.original_form('learn')\n",
    "\n",
    "# Note that you can also use stemming algorihtms from NLTK (nltk.stem package)\n",
    "# Source - http://www.nltk.org/howto/stem.html\n",
    "\n",
    "from __future__ import print_function\n",
    "from nltk.stem import *\n",
    "\n",
    "# Create a new Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Perform stemming to convert plural form to single form and change tenses to present tense\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))  # doctest: +NORMALIZE_WHITESPACE\n",
    "#caress fli die mule deni die agre own humbl size meet\n",
    "#state siez item sensat tradit refer colon plot\n",
    "\n",
    "# Create a new Snowball stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# See which languages are supported.\n",
    "print(\" \".join(SnowballStemmer.languages))\n",
    "#danish dutch english finnish french german hungarian italian\n",
    "#norwegian porter portuguese romanian russian spanish swedish\n",
    "\n",
    "#Create a new instance of a language specific subclass.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Stem a word.\n",
    "print(stemmer.stem(\"running\"))\n",
    "#run\n",
    "\n",
    "# Decide not to stem stopwords.\n",
    "stemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "print(stemmer.stem(\"having\"))\n",
    "#have\n",
    "\n",
    "print(stemmer2.stem(\"having\"))\n",
    "#having\n",
    "\n",
    "# Note that the 'english' stemmer is better than the original 'porter' stemmer.\n",
    "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
    "#generous\n",
    "\n",
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))\n",
    "#gener\n",
    "\n",
    "# Back to our case study\n",
    "#print(sentences[:20])\n",
    "\n",
    "sentences = [stemmer.stem(sentence) for sentence in sentences]\n",
    "print(sentences[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Run a simple Word2Vec model\n",
    "\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "\n",
    "# Give a try\n",
    "simple_ngram = [['semantha', 'bee'], ['semantha', 'gibb']]\n",
    "\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(simple_ngram, min_count=1)\n",
    "\n",
    "# check vocabularies used in the model\n",
    "vocab = model.wv.vocab.keys()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Build a Word2Vec model for our wikedia case study\n",
    "\n",
    "#Syntax: model = Word2Vec(sentences, min_count=min_count, size=size, window=window)\n",
    "#\n",
    "# Parameters: \n",
    "#\n",
    "# size: the size of the NN layers, which correspond to the “degrees” of freedom\n",
    "#           the training algorithm has. Bigger size values require more training \n",
    "#           data, but can lead to better (more accurate) models. Reasonable values \n",
    "#           are in the tens to hundreds (default = 100).\n",
    "#\n",
    "# window: only terms hat occur within a window-neighbourhood of a term, in a sentence, \n",
    "#           are associated with it during training. The usual value is 4. Unless your \n",
    "#           text contains big sentences, leave it at that.\n",
    "#\n",
    "# min_count: 0-100, depending on the size of the dataset of interest.\n",
    "#\n",
    "# workers: the number of devices of parallelization to speed up training.\n",
    "#           (default = 1 = no parallelization). This parameter has only effect \n",
    "#           if you have Cython installed. Without Cython, you’ll only be able \n",
    "#           to use one core because of the GIL (and word2vec training is slow).\n",
    "#\n",
    "# iter: the sweeps of SGD through the data; more is better. It runs in general iter+1 \n",
    "#          passes; by default iter=5. \n",
    "\n",
    "# Back to our wikipedia case study\n",
    "min_count = 100 # size = 200 [u'queen', u'elizabeth']\n",
    "min_count = 50   # size = 200 [u'queen', u'royal', u'elizabeth', u'british']\n",
    "min_count = 10\n",
    "size = 100\n",
    "window = 5\n",
    "\n",
    "# Save the sentences to a file\n",
    "outfile = open('/tmp/my_sentences.txt','w')\n",
    "\n",
    "# Compute skip gram\n",
    "my_sentences = []\n",
    "start = 0\n",
    "sentences = sentences_orig\n",
    "total = len(sentences)\n",
    "for index in range(start,total):\n",
    "    for jndex in range(max(index-window,start),min(index+window,total)): \n",
    "        if jndex < index:\n",
    "            ngram = [sentences[jndex], sentences[index]]\n",
    "        elif jndex > index:\n",
    "            ngram = [sentences[index], sentences[jndex]]\n",
    "    my_sentences.append(ngram)\n",
    "    outfile.write(str(ngram))\n",
    "#print(my_sentences[0:20])  \n",
    "\n",
    "# Save my_sentences\n",
    "my_sentences_orig = my_sentences\n",
    "    \n",
    "# Run a Word2Vec model    \n",
    "model = Word2Vec(my_sentences, min_count=min_count, size=size, window=window)\n",
    "\n",
    "# Save model. Note that you can also use model.wv.save_word2vec_format instead.\n",
    "fname = \"/tmp/wikipedia.model\"\n",
    "model.save(fname)\n",
    "model = Word2Vec.load(fname)\n",
    "\n",
    "# Check vocabularies used in the model\n",
    "vocab = model.wv.vocab.keys()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6. Retrieve saved sentences and run Word2Vec model\n",
    "\n",
    "# Load sentences\n",
    "inpfile = open('/tmp/my_sentences.txt','r')\n",
    "my_texts = inpfile.readlines()[0]\n",
    "my_texts = my_texts.replace(\"\\'\",\"'\")\n",
    "for texts in my_texts.split(']['):\n",
    "    cbow = []\n",
    "    for text in texts.split(','):\n",
    "        text = text.strip().replace(\"[\",\"\")\n",
    "        text = text.strip().replace(\"]\",\"\")\n",
    "        if len(text) > 0:\n",
    "            cbow.append(text)\n",
    "    if len(cbow) > 0:\n",
    "        my_sentences.append(cbow)\n",
    "#print(my_sentences[0:20])\n",
    "\n",
    "# Case 1. Read data and run with default parameters\n",
    "my_model_1 = Word2Vec(my_sentences) # generate different results\n",
    "\n",
    "# Check vocabularies used in the model\n",
    "my_vocab_1 = my_model_1.wv.vocab.keys()\n",
    "#print(my_vocab_1)\n",
    "\n",
    "# Case 2. Use our parameters defined in the previous session\n",
    "min_count = 20\n",
    "size = 100\n",
    "window = 5\n",
    "my_model_2 = Word2Vec(my_sentences_orig, min_count=min_count, size=size, window=window)\n",
    "\n",
    "# check vocabularies used in the model\n",
    "my_vocab_2 = my_model_2.wv.vocab.keys()\n",
    "#print(my_vocab_2)\n",
    "\n",
    "# Case 3. Manually build your model by calling Word2Vec(sentences, iter=1) to run \n",
    "# two passes over the sentences iterator. \n",
    "# 1. The first pass collects words and their frequencies to build an internal \n",
    "#    dictionary tree structure. \n",
    "# 2. The second and subsequent passes train the neural model. These two (or, \n",
    "#   iter+1) passes can also be initiated manually, in case your input stream \n",
    "#   is non-repeatable (you can only afford one pass), and you’re able to initialize \n",
    "#   the vocabulary some other way:\n",
    "\n",
    "my_model_3 = Word2Vec(iter=1)  # an empty model, no training yet\n",
    "my_model_3.build_vocab(my_sentences)  # can be a non-repeatable, 1-pass generator\n",
    "my_model_3.train(my_sentences,total_examples=my_model_3.corpus_count,epochs=my_model_3.iter) # can be a non-repeatable, 1-pass generator\n",
    "\n",
    "# check vocabularies used in the model\n",
    "my_vocab_3 = my_model_3.wv.vocab.keys()\n",
    "#print(my_vocab_3)\n",
    "\n",
    "# Case 4. Load the saved model\n",
    "fname = \"/tmp/wikipedia.model\"\n",
    "model = Word2Vec.load(fname)\n",
    "\n",
    "# check vocabularies used in the model\n",
    "vocab = model.wv.vocab.keys()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Perform similarity analysis\n",
    "# To compute the cosine similarity between two terms, use the similarity method. \n",
    "# Cosine similarity is generally bounded by [-1, 1]. The corresponding ‘distance’ \n",
    "# can be measured as 1-similarity. To figure out the terms most similar to a \n",
    "# special one, you can use the most_similar method.\n",
    "\n",
    "# Back to our wikipedia case study\n",
    "#min_count = 100 # size = 200 [u'queen', u'elizabeth']\n",
    "#min_count = 50   # size = 200 [u'queen', u'royal', u'elizabeth', u'british']\n",
    "#min_count = 20\n",
    "#size = 100\n",
    "#window = 5\n",
    "#model = Word2Vec(my_sentences, min_count=min_count, size=size, window=window, hs=1, negative=0)\n",
    "\n",
    "# check vocabularies used in the model\n",
    "#vocab = model.wv.vocab.keys()\n",
    "#print(vocab)\n",
    "\n",
    "print(1,StemmingHelper.stem('artists'))\n",
    "print(2,model.wv.most_similar(StemmingHelper.stem('artists')))\n",
    "print(3,model.wv.similarity('gaga', 'madonna'))\n",
    "print(4,model.wv.similarity('song', 'fashion'))\n",
    "print(5,model.wv.similarity('pop', 'woman'))\n",
    "print(6,model.wv.similarity('album', 'song'))\n",
    "print(7,model.wv.similarity('art', 'performance'))\n",
    "print(8,model.wv.similarity('song', 'video'))\n",
    "print(9,model.wv.similarity('gaga', 'grammy'))\n",
    "print(10,model.wv.similarity('madonna', 'grammy'))\n",
    "print(11,model.wv.most_similar(positive=['artist', 'gaga'], negative=['pop'])) \n",
    "print(12,model.wv.most_similar_cosmul(positive=['singer', 'songwriter'], negative=['song'])) \n",
    "print(13,model.wv.doesnt_match(\"Gaga was named the 'Queen of Pop' in a 2016 ranking by Rolling Stone\".lower().split()))\n",
    "print(14,model.wv.most_similar_cosmul(positive=['artist', 'producer'], negative=['performance'])) \n",
    "print(15,model.wv.doesnt_match(\"Gaga's vocal style was inspired by Madonna\".lower().split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8. Compute model score\n",
    "\n",
    "# gensim has currently only implemented score for the hierarchical softmax scheme [Mikolov et al., 2013], \n",
    "# so you should have run word2vec with hs=1 and negative=0 for this to work.\n",
    "\n",
    "# Back to our wikipedia case study\n",
    "#min_count = 100 # size = 200 [u'queen', u'elizabeth']\n",
    "#min_count = 50   # size = 200 [u'queen', u'royal', u'elizabeth', u'british']\n",
    "min_count = 10\n",
    "size = 100\n",
    "window = 5\n",
    "iter = 5 # iter = sweeps of SGD through the data; more is better (however, it takes a while. Please wait!)\n",
    "model = Word2Vec(my_sentences, min_count=min_count, iter=iter, size=size, window=window, hs=1, negative=0)\n",
    "\n",
    "# we only have scoring for the hierarchical softmax setup \n",
    "print(model.score([\"Gaga was named the 'Queen of Pop' in a 2011 ranking by Rolling Stone\".split()]))\n",
    "print(model.score([\"Gaga was named the 'Queen of Pop' in a 2016 ranking by Rolling Stone\".split()]))\n",
    "print(model.score([\"Gaga's vocal style was inspired by Madonna\".split()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
