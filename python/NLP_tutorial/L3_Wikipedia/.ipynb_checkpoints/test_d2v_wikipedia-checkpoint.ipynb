{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Apr 19 21:25:09 2017\n",
    "\n",
    "@author: raysun\n",
    "\n",
    "Sources - https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "          https://radimrehurek.com/gensim/wiki.html\n",
    "          https://markroxor.github.io/gensim/static/notebooks/doc2vec-lee.html\n",
    "          https://markroxor.github.io/gensim/static/notebooks/doc2vec-wikipedia.html\n",
    "          https://arxiv.org/abs/1507.07998\n",
    "\"\"\"\n",
    "\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1. Load the Wikipedia articles\n",
    "\n",
    "# Download the dump of all Wikipedia articles from http://download.wikimedia.org/enwiki/, where you may need either \n",
    "# 1. enwiki-latest-pages-articles.xml.bz2, or \n",
    "# 2. enwiki-YYYYMMDD-pages-articles.xml.bz2 for date-specific dumps. \n",
    "\n",
    "# This file is about 14GB in size and contains (a compressed version of) all articles from the English Wikipedia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2. Load the corpus\n",
    "\n",
    "# Load the wiki corpus\n",
    "wiki = WikiCorpus(\"/Users/raysun/test_data/examples/L3_Wikipedia/wiki/enwiki-latest-pages-articles.xml.bz2\")\n",
    "\n",
    "# Define TaggedWikiDocument class to convert WikiCorpus into suitable form for Doc2Vec\n",
    "class TaggedWikiDocument(object):\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "        self.wiki.metadata = True\n",
    "    def __iter__(self):\n",
    "        for content, (page_id, title) in self.wiki.get_texts():\n",
    "            yield TaggedDocument([c.decode(\"utf-8\") for c in content], [title])  \n",
    "\n",
    "# Generate tagged wiki documents\n",
    "documents = TaggedWikiDocument(wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3. Perform text processing to compute the optimized min_count\n",
    "\n",
    "# Preprocess text data\n",
    "preproc = Doc2Vec(min_count=0)\n",
    "preproc.scan_vocab(documents)\n",
    "\n",
    "# Optimize the min_count\n",
    "for num in range(0, 20):\n",
    "    print('min_count: {}, size of vocab: '.format(num), \n",
    "          pre.scale_vocab(min_count=num, dry_run=True)['memory']['vocab']/1000)\n",
    "\n",
    "# Print optimized min_count\n",
    "print(min_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 4. Build Doc2Vec models\n",
    "\n",
    "# Enable multirocessing by the number of available CPUs\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Build the Doc2Vec models\n",
    "models = [\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, dbow_words=1, size=200, window=8, min_count=19, iter=10, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=200, window=8, min_count=19, iter =10, workers=cores),\n",
    "]\n",
    "\n",
    "# Build vocabularies\n",
    "models[0].build_vocab(documents)\n",
    "print(str(models[0]))\n",
    "models[1].reset_from(models[0])\n",
    "print(str(models[1]))\n",
    "\n",
    "# Save models\n",
    "model.save('/tmp/d2v_wikipedia.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 5. Train Doc2Vec of the English wikipedia articles\n",
    "\n",
    "print models\n",
    "\n",
    "for model in models:\n",
    "    %%time model.train(documents,total_examples=model.corpus_count,epochs=model.iter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 6. Analyze similarity\n",
    "\n",
    "# First, we calculate cosine simillarity of \"Machine learning\" using Paragraph Vector. \n",
    "# Word Vector and Document Vector are separately stored. We have to add .docvecs after \n",
    "# model name to extract Document Vector from Doc2Vec Model.\n",
    "for model in models:\n",
    "    print(str(model))\n",
    "    pprint(model.docvecs.most_similar(positive=[\"Machine learning\"], topn=20))\n",
    "    \n",
    "# Note that DBOW model interpret the word 'Machine Learning' as a part of Computer Science \n",
    "# field, and DM model as Data Science related field.\n",
    "\n",
    "# Second, we calculate cosine simillarity of \"Lady Gaga\" using Paragraph Vector.\n",
    "for model in models:\n",
    "    print(str(model))\n",
    "    pprint(model.docvecs.most_similar(positive=[\"Lady Gaga\"], topn=10))\n",
    "    \n",
    "# Third, calculating cosine simillarity of \"Lady Gaga\" - \"American\" + \"Japanese\" - \"Italian\" using \n",
    "# Document vector and Word Vectors. \"American\" and \"Japanese\" are Word Vectors, not Paragraph \n",
    "# Vectors. Word Vectors are already converted to lowercases by WikiCorpus.\n",
    "for model in models:\n",
    "    print(str(model))\n",
    "    vec = [model.docvecs[\"Lady Gaga\"] - model[\"american\"] + model[\"japanese\"] - model[\"italian\"]]\n",
    "    pprint([m for m in model.docvecs.most_similar(vec, topn=11) if m[0] != \"Lady Gaga\"])\n",
    "    \n",
    "# As a result, DBOW model demonstrate the similar artists with Lady Gaga in Japan such as \n",
    "# 'Perfume', which is the Most famous Idol in Japan. On the other hand, DM model results \n",
    "# don't include the Japanese aritsts in top 10 simillar documents. It's almost same with \n",
    "# no vector calculated results.\n",
    "\n",
    "# This results demonstrate that DBOW employed in the original paper is outstanding for \n",
    "# calculating the similarity between Document Vector and Word Vector.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Option 1. Convert the Wikipedia articles (used for LSI and LDA analyses)\n",
    "\n",
    "# Convert the articles to plain text (process Wiki markup) and store the result as sparse TF-IDF vectors. \n",
    "# In Python, this is easy to do on-the-fly and we donâ€™t even need to uncompress the whole archive to disk. \n",
    "# There is a script included in gensim that does just that, run the command like:\n",
    "'''\n",
    "    python -m gensim.scripts.make_wikicorpus ./gensim/results/enwiki-latest-pages-articles.xml.bz2 ./gensim/results/wiki_en\n",
    "'''\n",
    "# This step takes several hours and uses about 30GB disk space.\n",
    "\n",
    "import logging, gensim\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "# Load id->word mapping (the dictionary), one of the results of step 2 above\n",
    "id2word = corpora.Dictionary.load_from_text('wiki_en_wordids.txt')\n",
    "\n",
    "# Load corpus iterator\n",
    "mm = corpora.MmCorpus('wiki_en_tfidf.mm')\n",
    "# mm = corpora.MmCorpus(bz2.BZ2File('wiki_en_tfidf.mm.bz2')) # use this if you compressed the TFIDF output\n",
    "print(mm)\n",
    "\n",
    "# a. Extract 100 LSI topics; use the default one-pass algorithm\n",
    "lsi = models.lsimodel.LsiModel(corpus=mm, id2word=id2word, num_topics=100)\n",
    "\n",
    "# Print the most contributing words (both positively and negatively) for each of the first ten topics\n",
    "print lsi.print_topics(10)\n",
    "\n",
    "# b. Extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents)\n",
    "lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1)\n",
    "\n",
    "# Print the most contributing words (both positively and negatively) for each of the first ten topics\n",
    "print lda.print_topics(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
