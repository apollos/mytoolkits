//Define all basic op of neural network generated by GA
//Same type of op with different parameter, such as kernel size, padding will share one message
//The parameter changes of same message may be controlled by mutation??!
syntax = "proto3";

package dlOp;

enum PADDING{
        SAME = 0;
        VALID = 1;
}

enum POOLING{
    AVERAGE = 0;
    MAX = 1;
}

enum ACTIVATION{
    RELU = 0;
    SELU = 1;
    SOFTPLUS = 2;
    SOFTSIGN = 3;
    SIGMOID = 4;
}

enum WEIGHT_INITIALIZERS{
    xavier = 0;
}

enum BIASES_INITIALIZERS{
    zero = 0;
}

enum LOSS_PREDICTION{
    predictions = 0; // it is a hack for constant value define in protobuf
}

enum LOSS_LOGITS{
    logits = 0; //it is a hack for constant value define in protobuf
}

message Conv2D_op
{
    int32     filters = 1 ;
    repeated int32     kernels = 2 ;//normally, it shall be two items (3, 3)
    repeated int32     strides = 3 ;//normally, it shall be two items (3, 3)
    PADDING   paddings = 4;
    bool      bias = 5;// [default = true];
    bool      trainable = 6;// [default = true];
    bool      last_op = 7;// [default = false];
}

message Softmax_Cross_Entropy
{
    LOSS_LOGITS type = 1;
    repeated float           weights = 2;
    bool            last_op = 3;//[default = true];
    int32           lable_smoothing = 4;// [default = 0];
}


message GenePool
{
    bool        filled = 1;
    repeated    Conv2D_op   conv2d_op_list = 2;
    repeated    Softmax_Cross_Entropy   softmax_cross_entropy_op_list = 16;
}